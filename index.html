<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
    <!-- Replace the content tag with appropriate information -->
    <meta name="description" content="Metrics in the future would provide not just the score but also the rationale, enabling the understanding of each judgment.">
    <meta property="og:title" content="VIEScore: Towards Explainable Metrics for Conditional Image Synthesis Evaluation" />
    <meta property="og:description" content="Metrics in the future would provide not just the score but also the rationale, enabling the understanding of each judgment." />
    <meta property="og:url" content="https://tiger-ai-lab.github.io/VIEScore/" />
    <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
    <meta property="og:image" content="static/images/teaser.png" />
    <meta property="og:image:width" content="1200" />
    <meta property="og:image:height" content="630" />


    <meta name="twitter:title" content="VIEScore: Towards Explainable Metrics for Conditional Image Synthesis Evaluation">
    <meta name="twitter:description" content="Metrics in the future would provide not just the score but also the rationale, enabling the understanding of each judgment.">
    <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
    <meta name="twitter:image" content="static/images/teaser.png">
    <meta name="twitter:card" content="summary_large_image">
    <!-- Keywords for your paper to be indexed by-->
    <meta name="keywords" content="VIEScore">
    <meta name="viewport" content="width=device-width, initial-scale=1">


    <title>VIEScore</title>
    <link rel="icon" type="image/x-icon" href="static/images/favicon_io/favicon.ico">
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

    <link rel="stylesheet" href="static/css/bulma.min.css">
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="static/css/index.css">

    <script src="static/js/jquery.min.js"></script>
    <script src="static/js/main.js"></script>
    <script defer src="static/js/fontawesome.all.min.js"></script>
    <script src="static/js/bulma-carousel.min.js"></script>
    <script src="static/js/bulma-slider.min.js"></script>
    <script src="static/js/index.js"></script>

    <link rel="stylesheet" type="text/css" href="static/css/jquery.dataTables.css">
    <script type="text/javascript" charset="utf8" src="static/js/jquery-3.5.1.js"></script>
    <script type="text/javascript" charset="utf8" src="static/js/jquery.dataTables.js"></script>
</head>

<body>


    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column has-text-centered">
                        <h1 class="title is-1 publication-title"> VIEScore: Towards Explainable Metrics for Conditional Image Synthesis Evaluation</h1>
                        <div class="is-size-5 publication-authors">
                            <!-- Paper authors -->
                            <span class="author-block">
                <sup>‚ô†Ô∏è</sup><a href="https://kuwingfung.github.io/" target="_blank">Max Ku</a>,</span>
                            <span class="author-block">
                  <sup>‚ô†Ô∏è</sup><a href="https://jdf-prog.github.io/" target="_blank">Dongfu Jiang</a>,
                            <span class="author-block">
                    <sup>‚ô†Ô∏è</sup><a href="https://congwei1230.github.io/" target="_blank">Cong Wei</a>,
                  </span>
                            <span class="author-block">
                    <sup>‚ô°</sup><a href="https://xiangyue9607.github.io/" target="_blank">Xiang Yue</a>,
                  </span>
                            <span class="author-block">
                    <sup>‚ô†Ô∏è</sup><a href="https://wenhuchen.github.io/" target="_blank">Wenhu Chen</a>
                  </span>
                        </div>



                        <div class="is-size-5 publication-authors">
                            <span class="author-block">
                    <sup>‚ô†Ô∏è</sup>University of Waterloo,
                    <sup>‚ô°</sup>IN.AI Research
                            </span>
                            <span class="author-block"><small>{m3ku, dongfu.jiang, c58wei}@uwaterloo.ca, xiangyue@in.ai, wenhu.chen@uwaterloo.ca</small></span>

                        </div>

                        <div class="column has-text-centered">
                            <div class="publication-links">

                                <!-- Github link -->
                                <span class="link-block">
                      <a href="https://github.com/TIGER-AI-Lab/VIEScore" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fab fa-github"></i>
                      </span>
                                <span>Code</span>
                                </a>
                                </span>

                                <!-- ArXiv abstract Link -->
                                <span class="link-block">
                    <a href="https://arxiv.org/abs/2312.14867" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="ai ai-arxiv"></i>
                      </span>
                                <span>arXiv</span>
                                </a>
                                </span>

                                <span class="link-block">
                                    <a href="https://tiger-ai-lab.github.io/ImagenHub/" target="_blank"
                                    class="external-link button is-normal is-rounded is-dark">
                                              <span>üñºÔ∏è ImagenHub</span>
                                              </a>
                                </span>
                                
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- Image carousel -->
    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column is-full">
                        <div class="item">
                            <!-- Your image here -->
                            <img src="static/images/teaser.png" alt="VIEScore" width="100%"/>
                            <h2 class="subtitle">
                                Figure 1: <b>Metrics in the future would provide not just the score but also the rationale, enabling the understanding of each judgment.</b> 
                                    Which method (VIEScore or traditional metrics) is ‚Äúcloser‚Äù to the human perspectives? 
                            </h2>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>
    <!-- End image carousel -->


    <!-- Paper abstract -->
    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column has-text-centered">
                        <h2 class="title is-3">Abstract</h2>
                        <div class="content has-text-justified">
                            <p>
                                In the rapidly advancing field of conditional image generation research, challenges such as limited explainability lie in effectively evaluating the performance and capabilities of various models. This paper introduces VIEScore, a Visual Instruction-guided Explainable metric for evaluating any conditional image generation tasks. VIEScore leverages general knowledge from Multimodal Large Language Models (MLLMs) as the backbone and does not require training or fine-tuning. We evaluate VIEScore on seven prominent tasks in conditional image tasks and found: (1) VIEScore (GPT4-v) achieves a high Spearman correlation of 0.3 with human evaluations, while the human-to-human correlation is 0.45. (2) VIEScore (with open-source MLLM) is significantly weaker than GPT-4v in evaluating synthetic images. (3) VIEScore achieves a correlation on par with human ratings in the generation tasks but struggles in editing tasks. With these results, we believe VIEScore shows its great potential to replace human judges in evaluating image synthesis tasks.
                            </p>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>
    <!-- End paper abstract -->


    <section class="hero is-small">
        <div class="hero-body">
            <div class="container  is-max-desktop">
                <div class="columns is-centered has-text-centered">
                    <div class="column is-four-fifths">
                        <h2 class="title is-3">How VIEScore works?</h2>
                        <div class="item">
                            <!-- Your image here -->
                            <p>
                                Traditional metrics lack task awareness! It also lacks reasoning ability. VIEScore tackles both downsides. In VIEScore, All input conditions, synthesized images, and rating instructions are fed together to the MLLM in one pass. Then we retrieve the score in any scale and the rationale.
                            </p>
                            <img src="static/images/method.png" alt="MY ALT TEXT" />
                            <h2 class="subtitle has-text-centered">
                                Figure 2: Process of MLLM evaluation on one synthetic image in VIEScore. All input conditions, synthesized images, and rating instructions are fed together to the MLLM in one pass. Multi-concept image composition task is used here as an example.
                            </h2>
                            <p>To evaluate the effectiveness of our method, we used the 7 conditional Image generation tasks from ImagenHub and computed the correlation with human ratings collected in ImagenHub.</p>
                            <img src="static/images/intro_new.png" alt="MY ALT TEXT" />
                            <h2 class="subtitle has-text-centered">
                                Figure 3: We study the correlation between MLLMs
                                and human perspectives on rating images across all tasks in ImagenHub.
                            </h2>
                            <p>
                                
                            </p>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>


    <section class="hero is-small">
        <div class="hero-body">
            <div class="container  is-max-desktop">
                <div class="columns is-centered has-text-centered">
                    <div class="column is-four-fifths">
                        <h2 class="title is-3">How effective is VIEScore with current state-of-the-art MLLMs?</h2>
                        <div class="item">
                            <p>But how well can Multimodal large language models access different tasks of conditional Image generation? We reported that the best model GPT4v‚Äôs performance is significantly better than the open-source models. Most open-source MLLMs failed to adapt to our VieScore except LLaVA.</p>
                            <!-- Your image here -->
                            <img src="static/images/table_overall.png" alt="MY ALT TEXT" />
                            <h2 class="subtitle">
                                Table 1: Correlations across all tasks with different backbone models. We highlight the highest correlation numbers in green. Visit our paper for insights and challenges in VIEScore.
                            </h2>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="hero is-small">
        <div class="hero-body">
            <div class="container  is-max-desktop">
                <div class="columns is-centered has-text-centered">
                    <div class="column is-four-fifths">
                        <h2 class="title is-3">How is Traditional Metrics correlating with human compare to VIEScore?</h2>
                        <p>Looking into the details, we found that GPT4v achieves on par with human ratings on text-to-image task but it straggles on image editing tasks. We also compared with the traditional metrics.</p>
                        <div class="item">
                            <div class="content has-text-justified">
                                <table>
                                    <thead>
                                        <tr>
                                            <th>Method</th>
                                            <th>Method-Human<sup>SC</sup><sub>corr</sub></th>
                                            <th>Method-Human<sup>PQ</sup><sub>corr</sub></th>
                                            <th>Method-Human<sup>O</sup><sub>corr</sub></th>
                                        </tr>
                                    </thead>
                                    <tbody id="tabResults">
                                        <tr class="th">
                                            <td id="T2I" colspan="4" style="text-align: center; font-weight: bold;">Text-guided Image Generation Model (5 models)</td>
                                        </tr>
                                        <tr>
                                            <td style="color:gray; font-weight: bold;">Human Raters </td>
                                            <td>-</td>
                                            <td>Unknown</td>
                                            <td style="color:gray; font-weight: bold;">0.5044</td>
                                            <td style="color:gray; font-weight: bold;">0.3640</td>
                                            <td style="color:gray; font-weight: bold;">0.4652</td>
                                        </tr>
                                        <tr>
                                            <td>CLIP-Score</td>
                                            <td>-0.0817</td>
                                            <td>-0.0114</td>
                                            <td>-0.0881</td>
                                        </tr>
                                        <tr>
                                            <td style="color:blue; font-weight: bold;">VIEScore(GPT-4v<sub>0shot</sub>)</td>
                                            <td style="color:black; font-weight: bold;">0.4885</td>
                                            <td style="color:black; font-weight: bold;">0.2379</td>
                                            <td style="color:blue; font-weight: bold;">0.4614</td>
                                        </tr>
                                        <tr>
                                            <td>VIEScore(GPT-4v<sub>1shot</sub>)</td>
                                            <td>0.4531</td>
                                            <td>0.1770</td>
                                            <td>0.3801</td>
                                        </tr>
                                        <tr>
                                            <td>VIEScore(LLaVA<sub>0shot</sub>)</td>
                                            <td>0.1809</td>
                                            <td>0.0306</td>
                                            <td>0.1410</td>
                                        </tr>
                                        <tr>
                                            <td>VIEScore(LLaVA<sub>1shot</sub>)</td>
                                            <td>0.1789</td>
                                            <td>-0.0020</td>
                                            <td>0.1309</td>
                                        </tr>
                                        <tr class="th">
                                            <td id="MIE" colspan="4" style="text-align: center; font-weight: bold;">Mask-guided Image Editing Model (4 models)</td>
                                        </tr>
                                        <tr>
                                            <td style="color:gray; font-weight: bold;">Human Raters </td>
                                            <td style="color:gray; font-weight: bold;">0.5390</td>
                                            <td style="color:gray; font-weight: bold;">0.5030</td>
                                            <td style="color:gray; font-weight: bold;">0.4981</td>
                                        </tr>
                                        <tr>
                                            <td>LPIPS</td>
                                            <td>-0.1012</td>
                                            <td>0.0646</td>
                                            <td>-0.0694</td>
                                        </tr>
                                        <tr>
                                            <td style="color:blue; font-weight: bold;">VIEScore(GPT-4v<sub>0shot</sub>)</td>
                                            <td style="color:black; font-weight: bold;">0.4508</td>
                                            <td style="color:black; font-weight: bold;">0.2859</td>
                                            <td style="color:blue; font-weight: bold;">0.4069</td>
                                        </tr>
                                        <tr>
                                            <td>VIEScore(GPT-4v<sub>1shot</sub>)</td>
                                            <td>0.4088</td>
                                            <td>0.2352</td>
                                            <td>0.3810</td>
                                        </tr>
                                        <tr>
                                            <td>VIEScore(LLaVA<sub>0shot</sub>)</td>
                                            <td>0.1180</td>
                                            <td>-0.0531</td>
                                            <td>0.0675</td>
                                        </tr>
                                        <tr>
                                            <td>VIEScore(LLaVA<sub>1shot</sub>)</td>
                                            <td>0.1263</td>
                                            <td>-0.0145</td>
                                            <td>0.1040</td>
                                        </tr>
                                        <tr class="th">
                                            <td id="TIE" colspan="4" style="text-align: center; font-weight: bold;">Text-guided Image Editing Model (8 models)</td>
                                        </tr>
                                        <tr>
                                            <td style="color:gray; font-weight: bold;">Human Raters </td>
                                            <td style="color:gray; font-weight: bold;">0.4230</td>
                                            <td style="color:gray; font-weight: bold;">0.5052</td>
                                            <td style="color:gray; font-weight: bold;">0.4184</td>
                                        </tr>
                                        <tr>
                                            <td>LPIPS</td>
                                            <td>0.0956</td>
                                            <td>0.2504</td>
                                            <td>0.1142</td>
                                        </tr>
                                        <tr>
                                            <td style="color:blue; font-weight: bold;">VIEScore(GPT-4v<sub>0shot</sub>)</td>
                                            <td style="color:black; font-weight: bold;">0.2610</td>
                                            <td style="color:black; font-weight: bold;">0.4274</td>
                                            <td style="color:blue; font-weight: bold;">0.2456</td>
                                        </tr>
                                        <tr>
                                            <td>VIEScore(GPT-4v<sub>1shot</sub>)</td>
                                            <td>0.2428</td>
                                            <td>0.3402</td>
                                            <td>0.2279</td>
                                        </tr>
                                        <tr>
                                            <td>VIEScore(LLaVA<sub>0shot</sub>)</td>
                                            <td>0.0448</td>
                                            <td>0.0583</td>
                                            <td>0.0273</td>
                                        </tr>
                                        <tr>
                                            <td>VIEScore(LLaVA<sub>1shot</sub>)</td>
                                            <td>0.0185</td>
                                            <td>-0.0107</td>
                                            <td>0.0258</td>
                                        </tr>
                                        <tr class="th">
                                            <td id="SDIG" colspan="4" style="text-align: center; font-weight: bold;">Subject-driven Image Generation Model (4 models)</td>
                                        </tr>
                                        <tr>
                                            <td style="color:gray; font-weight: bold;">Human Raters </td>
                                            <td style="color:gray; font-weight: bold;">0.4780</td>
                                            <td style="color:gray; font-weight: bold;">0.3565</td>
                                            <td style="color:gray; font-weight: bold;">0.4653</td>
                                        </tr>
                                        <tr>
                                            <td style="color:blue; font-weight: bold;">DINO</td>
                                            <td style="color:black; font-weight: bold;">0.4160</td>
                                            <td>0.1206</td>
                                            <td style="color:blue; font-weight: bold;">0.4246</td>
                                        </tr>
                                        <tr>
                                            <td>CLIP-I</td>
                                            <td>0.2961</td>
                                            <td>0.1694</td>
                                            <td>0.3058</td>
                                        </tr>
                                        <tr>
                                            <td>VIEScore(GPT-4v<sub>0shot</sub>)</td>
                                            <td>0.3979</td>
                                            <td>0.1903</td>
                                            <td>0.3738</td>
                                        </tr>
                                        <tr>
                                            <td>VIEScore(GPT-4v<sub>1shot</sub>)</td>
                                            <td>0.2757</td>
                                            <td style="color:black; font-weight: bold;">0.2261</td>
                                            <td>0.2753</td>
                                        </tr>
                                        <tr>
                                            <td>VIEScore(LLaVA<sub>0shot</sub>)</td>
                                            <td>0.0326</td>
                                            <td>-0.0303</td>
                                            <td>0.1219</td>
                                        </tr>
                                        <tr>
                                            <td>VIEScore(LLaVA<sub>1shot</sub>)</td>
                                            <td>0.1334</td>
                                            <td>0.0858</td>
                                            <td>0.1248</td>
                                        </tr>
                                        <tr class="th">
                                            <td id="SDIE" colspan="4" style="text-align: center; font-weight: bold;">Subject-driven Image Editing Model (3 models)</td>
                                        </tr>
                                        <tr>
                                            <td style="color:gray; font-weight: bold;">Human Raters </td>
                                            <td style="color:gray; font-weight: bold;">0.4887</td>
                                            <td style="color:gray; font-weight: bold;">0.2986</td>
                                            <td style="color:gray; font-weight: bold;">0.4747</td>
                                        </tr>
                                        <tr>
                                            <td style="color:blue; font-weight: bold;">DINO</td>
                                            <td>0.3022</td>
                                            <td>-0.0381</td>
                                            <td style="color:blue; font-weight: bold;">0.3005</td>
                                        </tr>
                                        <tr>
                                            <td>CLIP-I</td>
                                            <td>0.2834</td>
                                            <td>0.1248</td>
                                            <td>0.2813</td>
                                        </tr>
                                        <tr>
                                            <td>VIEScore(GPT-4v<sub>0shot</sub>)</td>
                                            <td style="color:black; font-weight: bold;">0.3274</td>
                                            <td style="color:black; font-weight: bold;">0.2960</td>
                                            <td>0.1507</td>
                                        </tr>
                                        <tr>
                                            <td>VIEScore(GPT-4v<sub>1shot</sub>)</td>
                                            <td>-0.0255</td>
                                            <td>0.1572</td>
                                            <td>-0.0139</td>
                                        </tr>
                                        <tr>
                                            <td>VIEScore(LLaVA<sub>0shot</sub>)</td>
                                            <td>0.0360</td>
                                            <td>-0.0073</td>
                                            <td>0.0168</td>
                                        </tr>
                                        <tr>
                                            <td>VIEScore(LLaVA<sub>1shot</sub>)</td>
                                            <td>0.0587</td>
                                            <td>-0.0249</td>
                                            <td>0.0309</td>
                                        </tr>
                                        <tr class="th">
                                            <td id="MCIC" colspan="4" style="text-align: center; font-weight: bold;">Multi-concept Image Composition Model (3 models)</td>
                                        </tr>
                                        <tr>
                                            <td style="color:gray; font-weight: bold;">Human Raters </td>
                                            <td style="color:gray; font-weight: bold;">0.5927</td>
                                            <td style="color:gray; font-weight: bold;">0.5145</td>
                                            <td style="color:gray; font-weight: bold;">0.5919</td>
                                        </tr>
                                        <tr>
                                            <td>DINO</td>
                                            <td>0.0979</td>
                                            <td>-0.1643</td>
                                            <td>0.0958</td>
                                        </tr>
                                        <tr>
                                            <td>CLIP-I</td>
                                            <td>0.1512</td>
                                            <td>-0.0963</td>
                                            <td>0.1498</td>
                                        </tr>
                                        <tr>
                                            <td style="color:blue; font-weight: bold;">VIEScore(GPT-4v<sub>0shot</sub>)</td>
                                            <td style="color:black; font-weight: bold;">0.3209</td>
                                            <td style="color:black; font-weight: bold;">0.3025</td>
                                            <td style="color:blue; font-weight: bold;">0.3346</td>
                                        </tr>
                                        <tr>
                                            <td>VIEScore(GPT-4v<sub>1shot</sub>)</td>
                                            <td>0.1859</td>
                                            <td>0.1185</td>
                                            <td>0.1918</td>
                                        </tr>
                                        <tr>
                                            <td>VIEScore(LLaVA<sub>0shot</sub>)</td>
                                            <td>0.1022</td>
                                            <td>0.1194</td>
                                            <td>0.1070</td>
                                        </tr>
                                        <tr>
                                            <td>VIEScore(LLaVA<sub>1shot</sub>)</td>
                                            <td>0.0828</td>
                                            <td>0.0379</td>
                                            <td>0.0293</td>
                                        </tr>
                                        <tr class="th">
                                            <td id="CIG" colspan="4" style="text-align: center; font-weight: bold;">Control-guided Image Generation Model (2 models)</td>
                                        </tr>
                                        <tr>
                                            <td style="color:gray; font-weight: bold;">Human Raters </td>
                                            <td style="color:gray; font-weight: bold;">0.5443</td>
                                            <td style="color:gray; font-weight: bold;">0.5279</td>
                                            <td style="color:gray; font-weight: bold;">0.5307</td>
                                        </tr>
                                        <tr>
                                            <td>LPIPS</td>
                                            <td>0.3699</td>
                                            <td>0.4204</td>
                                            <td>0.4133</td>
                                        </tr>
                                        <tr>
                                            <td>VIEScore(GPT-4v<sub>0shot</sub>)</td>
                                            <td style="color:black; font-weight: bold;">0.4360</td>
                                            <td style="color:black; font-weight: bold;">0.4975</td>
                                            <td>0.3999</td>
                                        </tr>
                                        <tr>
                                            <td style="color:blue; font-weight: bold;">VIEScore(GPT-4v<sub>1shot</sub>)</td>
                                            <td>0.3892</td>
                                            <td>0.4132</td>
                                            <td style="color:blue; font-weight: bold;">0.4237</td>
                                        </tr>
                                        <tr>
                                            <td>VIEScore(LLaVA<sub>0shot</sub>)</td>
                                            <td>0.2207</td>
                                            <td>0.1060</td>
                                            <td>0.1679</td>
                                        </tr>
                                        <tr>
                                            <td>VIEScore(LLaVA<sub>1shot</sub>)</td>
                                            <td>0.1121</td>
                                            <td>0.0247</td>
                                            <td>0.0416</td>
                                        </tr>
                            </div>
                            </tbody>
                            </table>
                            
                            <p>
                                Table 2: Correlations comparison of available methods. We highlight the best method and the correlation numbers closest to human raters. To conclude, VIEScore is the best metric in evaluating synthetic images across all tasks with high potential. DINO on the other hand proves to be an effective metric in Subject-Driven image generation and editing tasks.
                            </p>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>


    <section class="hero is-small">
        <div class="hero-body">
            <div class="container  is-max-desktop">
                <div class="columns is-centered has-text-centered">
                    <div class="column is-four-fifths">
                        <h2 class="title is-3">MLLM struggles on rating image editing tasks</h2>
                        <p>Why MLLMs struggle on rating image editing tasks? This marked the disability of MLLM‚Äôs evaluation on multiple images, as MLLMs are often confused with multiple images, such as failing to spot the difference between two images. But we believe VieScore will be a mainstream evaluation method in the near future, as in practice multiple image QA is an area coming with many applications and the MLLMs will be improved in the future.</p>
                        <div class="item">
                            <!-- Your image here -->
                            <img src="static/images/figure4.png" alt="MY ALT TEXT" width="45%"/>
                            <img src="static/images/figure5.png" alt="MY ALT TEXT" width="45%"/>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- BibTex citation -->
    <section class="section" id="BibTeX">
        <div class="container is-max-desktop content">
            <h2 class="title">Citation</h2>
            Please kindly cite our paper if you use our code, data, models or results:
            <br><br>
            <pre><code>@misc{ku2023viescore,
                title={VIEScore: Towards Explainable Metrics for Conditional Image Synthesis Evaluation}, 
                author={Max Ku and Dongfu Jiang and Cong Wei and Xiang Yue and Wenhu Chen},
                year={2023},
                eprint={2312.14867},
                archivePrefix={arXiv},
                primaryClass={cs.CV}
            }
            </code></pre>
        </div>
    </section>


    <footer class="footer">
        <div class="container">
            <div class="columns is-centered">
                <div class="column is-8">
                    <div class="content">

                        <p>
                            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a>                            project page. You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/"
                                target="_blank">Creative
              Commons Attribution-ShareAlike 4.0 International License</a>.
                        </p>

                    </div>
                </div>
            </div>
        </div>
    </footer>

</body>
<style>
    .buttonGroup {
        text-align: center;
    }
    
    .buttonGroup>button {
        padding: 15px;
        color: white;
        background-color: #363636;
        border-radius: 5px;
    }
    
    .buttonGroup>button:hover {
        box-shadow: 5px;
    }
</style>

</html>
